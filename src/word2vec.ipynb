{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download hebrew dataset from wikipedia\n",
    "   - Go to: https://dumps.wikimedia.org/hewiki/latest/\n",
    "   - Download `hewiki-latest-pages-articles.xml.bz2`\n",
    "   \n",
    "   In linux this can be easily done using: \n",
    "   \n",
    "   wget https://dumps.wikimedia.org/hewiki/latest/hewiki-latest-pages-articles.xml.bz2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import bz2\n",
    "import logging\n",
    "import multiprocessing\n",
    "\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "SCRIPT_PATH = os.path.dirname(os.path.realpath(__file__))\n",
    "DATA_PATH   = os.path.join(SCRIPT_PATH, 'data/')\n",
    "MODEL_PATH  = os.path.join(SCRIPT_PATH, 'word2vec/')\n",
    "\n",
    "DICTIONARY_FILEPATH = os.path.join(DATA_PATH, 'wiki-hebrew_wordids.txt.bz2')\n",
    "WIKI_DUMP_FILEPATH = os.path.join(DATA_PATH, 'hewiki-latest-pages-articles.xml.bz2')\n",
    "\n",
    "# Check if the required files have been downloaded\n",
    "if not WIKI_DUMP_FILEPATH:\n",
    "    print('Wikipedia articles dump could not be found..')\n",
    "    print('Please see README.md for instructions!')\n",
    "    sys.exit()\n",
    "\n",
    "# Get number of available cpus\n",
    "cores = multiprocessing.cpu_count()\n",
    "if not os.path.exists(MODEL_PATH):\n",
    "    os.makedirs(MODEL_PATH)\n",
    "\n",
    "# Initialize logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if not os.path.isfile(DICTIONARY_FILEPATH):\n",
    "        logging.info('Dictionary has not been created yet..')\n",
    "        logging.info('Creating dictionary (takes about 9h)..')\n",
    "\n",
    "        # Construct corpus\n",
    "        wiki = gensim.corpora.WikiCorpus(WIKI_DUMP_FILEPATH)\n",
    "\n",
    "        # Remove words occuring less than 20 times, and words occuring in more\n",
    "        # than 10% of the documents. (keep_n is the vocabulary size)\n",
    "        wiki.dictionary.filter_extremes(no_below=20, no_above=0.1, keep_n=100000)\n",
    "\n",
    "        # Save dictionary to file\n",
    "        wiki.dictionary.save_as_text(DICTIONARY_FILEPATH)\n",
    "        del wiki\n",
    "\n",
    "    # Load dictionary from file\n",
    "    dictionary = gensim.corpora.Dictionary.load_from_text(DICTIONARY_FILEPATH)\n",
    "\n",
    "    # Construct corpus using dictionary\n",
    "    wiki = gensim.corpora.WikiCorpus(WIKI_DUMP_FILEPATH, dictionary=dictionary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class SentencesIterator:\n",
    "        def __init__(self, wiki):\n",
    "            self.wiki = wiki\n",
    "\n",
    "        def __iter__(self):\n",
    "            for sentence in self.wiki.get_texts():\n",
    "                yield list(map(lambda x: x, sentence))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initialize simple sentence iterator required for the Word2Vec model\n",
    "sentences = SentencesIterator(wiki)\n",
    "logging.info('Training word2vec model..')\n",
    "model = gensim.models.Word2Vec(sentences=sentences, workers=cores)\n",
    "# Save model\n",
    "logging.info('Saving model..')\n",
    "model.save(os.path.join(MODEL_PATH, 'word2vec.model'))\n",
    "logging.info('Done training word2vec model!')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
